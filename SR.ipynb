{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ITSRN.code import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config loaded.\n"
     ]
    }
   ],
   "source": [
    "global config\n",
    "with open(\"ITSRN\\code\\configs\\\\train\\\\train_itnsr.yaml\", 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print('config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    \"\"\"\n",
    "    Make coordinates at grid centers.\n",
    "    \"\"\"\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()  # n coordinates between v0 and v1\n",
    "        coord_seqs.append(seq)\n",
    "\n",
    "    # Make coordinate grid\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs, indexing='ij'), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.reshape(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "def get_coord(img, scale):\n",
    "    \"\"\"\n",
    "    Get coordinates for the given image and target scale.\n",
    "\n",
    "    Args:\n",
    "        img: Input image tensor of shape (B, C, H, W)\n",
    "        scale: Target upsampling scale\n",
    "\n",
    "    Returns:\n",
    "        coord: Coordinate tensor\n",
    "    \"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "\n",
    "    # Calculate target size\n",
    "    H_up = int(H * scale)\n",
    "    W_up = int(W * scale)\n",
    "\n",
    "    # Generate normalized coordinates for target size\n",
    "    coord = make_coord((H_up, W_up))  # (H_up*W_up, 2)\n",
    "\n",
    "    # Add batch dimension if needed\n",
    "    if B > 1:\n",
    "        coord = coord.unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense(nChannels, growthRate):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1, bias=False),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class RDB(nn.Module):\n",
    "    def __init__(self, nChannels, nDenselayer, growthRate):\n",
    "        super(RDB, self).__init__()\n",
    "        nChannels_ = nChannels\n",
    "        modules = []\n",
    "        for i in range(nDenselayer):\n",
    "            modules.append(make_dense(nChannels_, growthRate))\n",
    "            nChannels_ += growthRate\n",
    "        self.dense_layers = nn.ModuleList(modules)\n",
    "        self.conv_1x1 = nn.Conv2d(nChannels_, nChannels, kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.dense_layers:\n",
    "            out = layer(torch.cat(features, 1))\n",
    "            features.append(out)\n",
    "        out = self.conv_1x1(torch.cat(features, 1))\n",
    "        return out + x\n",
    "\n",
    "class EnhancedFeatureExtraction(nn.Module):\n",
    "    def __init__(self, input_channels=3, base_channels=64, num_rdb=2, num_dense_layers=6, growth_rate=32):\n",
    "        super(EnhancedFeatureExtraction, self).__init__()\n",
    "\n",
    "        # Shallow feature extraction\n",
    "        self.sfe1 = nn.Conv2d(input_channels, base_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.sfe2 = nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # RDB blocks for local feature extraction\n",
    "        self.rdbs = nn.ModuleList([\n",
    "            RDB(base_channels, num_dense_layers, growth_rate)\n",
    "            for _ in range(num_rdb)\n",
    "        ])\n",
    "\n",
    "        # Enhanced Feature Fusion\n",
    "        total_rdb_channels = base_channels * (num_rdb + 1)  # +1 for initial features\n",
    "\n",
    "        # First fusion layer - reduces to half of total dimensions\n",
    "        self.fusion1 = nn.Sequential(\n",
    "            nn.Conv2d(total_rdb_channels, total_rdb_channels // 2, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Fusion-enhanced layer - further reduces dimension to base_channels\n",
    "        self.fusion2 = nn.Sequential(\n",
    "            nn.Conv2d(total_rdb_channels // 2, base_channels, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Context Enhancement Block\n",
    "        self.context_enhancement = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=4, dilation=4, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final conv\n",
    "        self.conv_out = nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shallow feature extraction\n",
    "        sfe1 = self.sfe1(x)\n",
    "        sfe2 = self.sfe2(sfe1)\n",
    "\n",
    "        # Local feature extraction with RDBs\n",
    "        rdb_in = sfe2\n",
    "        local_features = [sfe2]  # Include shallow features\n",
    "\n",
    "        for rdb in self.rdbs:\n",
    "            rdb_out = rdb(rdb_in)\n",
    "            local_features.append(rdb_out)\n",
    "            rdb_in = rdb_out\n",
    "\n",
    "        # Enhanced Feature Fusion\n",
    "        # 1. Concatenate all local features\n",
    "        concat_features = torch.cat(local_features, 1)\n",
    "        # 2. First fusion - reduce to half dimension\n",
    "        fused1 = self.fusion1(concat_features)\n",
    "        # 3. Enhanced fusion - further reduce dimension\n",
    "        fused2 = self.fusion2(fused1)\n",
    "\n",
    "        # Context enhancement\n",
    "        enhanced = self.context_enhancement(fused2)\n",
    "\n",
    "        # Final output\n",
    "        out = self.conv_out(enhanced)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFESR(nn.Module):\n",
    "    def __init__(self, input_channels=3, base_channels=64, num_rdb=2, num_dense_layers=6, growth_rate=32):\n",
    "        super(FFESR, self).__init__()\n",
    "        self.enhanced_feature_extraction = EnhancedFeatureExtraction(input_channels, base_channels, num_rdb, num_dense_layers, growth_rate)\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, 32, kernel_size=3, padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1, bias=True)\n",
    "        )\n",
    "        self.itsrn = models.make(config['model'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enhanced_feature_extraction(x)\n",
    "        y = self.itsrn(x, get_coord(x, 2), 2)\n",
    "        z = self.output_conv(x)\n",
    "        return (y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m, in \u001b[0;36mFFESR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menhanced_feature_extraction(x)\n\u001b[1;32m---> 16\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitsrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_coord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv(x)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (y, z)\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\ITSRN\\code\\models\\ITNSR.py:208\u001b[0m, in \u001b[0;36mITNSR.forward\u001b[1;34m(self, inp, coord, scale)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, coord, scale):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_feat(inp)\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\ITSRN\\code\\models\\ITNSR.py:80\u001b[0m, in \u001b[0;36mITNSR.query_rgb\u001b[1;34m(self, coord, scale)\u001b[0m\n\u001b[0;32m     76\u001b[0m     feat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39munfold(feat, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m     77\u001b[0m         feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m9\u001b[39m, feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# K\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m feat_coord \u001b[38;5;241m=\u001b[39m \u001b[43mmake_coord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \\\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39mfeat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# enhance local features\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_ensemble:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# v_lst = [(-1,-1),(-1,0),(-1,1),(0, -1), (0, 0), (0, 1),(1, -1),(1, 0),(1,1)]#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\suhas\\OneDrive\\Documents\\CS\\Acad\\sem7\\DIP_Project\\dip_env\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = FFESR(\n",
    "        input_channels=3,\n",
    "        base_channels=3,\n",
    "        num_rdb=3,\n",
    "        num_dense_layers=6,\n",
    "        growth_rate=32\n",
    "    )\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "# Forward pass\n",
    "output = model(sample_input)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output[0].shape}, {output[1].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
